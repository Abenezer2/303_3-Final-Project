{
 "cells": [
  {
   "cell_type": "raw",
   "id": "33dd6c4c",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Project Report\"\n",
    "subtitle: Team name\n",
    "author: Nathan Jung, Abenezer Bekele, Jack O'Keefe\n",
    "date: 05/23/2023\n",
    "number-sections: true\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    self-contained: true\n",
    "    font-size: 100%\n",
    "    toc-depth: 4\n",
    "    mainfont: serif\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0116f49b",
   "metadata": {},
   "source": [
    "## Background / Motivation\n",
    "\n",
    "What motivated you to work on this problem?\n",
    "\n",
    "Mention any background about the problem, if it is required to understand your analysis later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdb1dc",
   "metadata": {},
   "source": [
    "As the internet gets more popular, it is important for article creators to find the best way to make sure their articles reach the general population in order for their business to grow and for user retention to stay maximized. Being able to predict the popularity of an article would help authors or article-sites decide whether a particular topic or article is worth being published depending on the predicted number of shares from our created model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff1421",
   "metadata": {},
   "source": [
    "## Problem statement \n",
    "\n",
    "Describe your problem statement. Articulate your objectives using absolutely no jargon. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e15e8",
   "metadata": {},
   "source": [
    "We aim to predict an article's popularity, through the number of shares it has, through the particular article's characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7b95f",
   "metadata": {},
   "source": [
    "## Data sources\n",
    "What data did you use? Provide details about your data. Include links to data if you are using open-access data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81137e3d",
   "metadata": {},
   "source": [
    "Our data was gotten from an online open-source database about machine learning repositories. The data can be gotten from this [link.](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity) The data comprises of quantifiable characteristics about the article such as the number of words, day it was published, or rate of positive words in the article. There are two non-predictive variables (url and days between publish and data acquisition), leaving a total of 57 variables that can be used to predict the number of shares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c255035",
   "metadata": {},
   "source": [
    "## Stakeholders\n",
    "Who cares? If you are successful, what difference will it make to them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7436595",
   "metadata": {},
   "source": [
    "Publishers and Editors are interested in increasing the popularity of the articles and papers they publish. Thus, being able to predict an article’s popularity will help them make editorial decisions that maximize readership and engagement.<br>\n",
    "Authors and Researchers are interested in improving the reach and impact of their work, and by being able to predict their work’s popularity, they can optimize their writing and research to attract more readers. <br>\n",
    "Content Marketers and PR Professionals are responsible for promoting content and ensuring it reaches the target audience. Being able to predict a paper’s popularity will help them in crafting effective marketing strategies and public relations campaigns to maximize visibility and reach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7ea9bb",
   "metadata": {},
   "source": [
    "## Data quality check / cleaning / preparation \n",
    "\n",
    "Show the distribution of the response here. Report the standard deviation and mean in case of a regression problem, and proportion of 0s and 1s in case of classification.\n",
    "\n",
    "For all other content, as mentioned below, just provide the highlights *(if any)* and put the details in the appendix.\n",
    "\n",
    "In a tabular form, show the distribution of values of each variable used in the analysis - for both categorical and continuous variables. Distribution of a categorical variable must include the number of missing values, the number of unique values, the frequency of all its levels. If a categorical variable has too many levels, you may just include the counts of the top 3-5 levels. \n",
    "\n",
    "Mention any useful insights you obtained from the data quality check that helped you develop the model or helped you realize the necessary data cleaning / preparation. Its ok if there were none.\n",
    "\n",
    "Were there any potentially incorrect values of variables that required cleaning? If yes, how did you clean them? Were there missing values? How did you handle them? Its ok if the data was already clean.\n",
    "\n",
    "Did you do any data wrangling or data preparation before the data was ready to use for model development? Did you create any new predictors from exisiting predictors? For example, if you have number of transactions and spend in a credit card dataset, you may create spend per transaction for predicting if a customer pays their credit card bill. Mention the steps at a broad level, you may put minor details in the appendix. Only mention the steps that ended up being useful towards developing your model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d861e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score,train_test_split, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error,r2_score,roc_curve,auc,precision_recall_curve, accuracy_score, \\\n",
    "recall_score, precision_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid, StratifiedKFold\n",
    "from sklearn.ensemble import VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier, GradientBoostingRegressor,GradientBoostingClassifier, BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier,AdaBoostRegressor,AdaBoostClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import itertools as it\n",
    "import time as time\n",
    "import xgboost as xgb\n",
    "from pyearth import Earth\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6892f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "data = pd.read_csv('../OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "data.columns = [x.strip() for x in data.columns.tolist()]\n",
    "X = data.drop(columns = ['url', 'timedelta', 'shares'])\n",
    "y = data['shares']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d311d46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are predicting shares. \n",
      "The mean of shares is 3395.3801836343455. The standard deviation of shares is 11626.950748651712.\n",
      "From the distribution of shares, it was evident that it was quite necessary make a log transformation on shares.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1300x350 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| echo: false\n",
    "print(\"We are predicting shares. \\nThe mean of shares is {}. The standard deviation of shares is {}.\".format(data.shares.mean(), data.shares.std()))\n",
    "print(\"From the distribution of shares, it was evident that it was quite necessary make a log transformation on shares.\")\n",
    "plt.rcParams[\"figure.figsize\"] = [13.00, 3.50]\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "num1 = sns.histplot(data.shares, ax = axes[0])\n",
    "num2 = sns.histplot(data.shares[data.shares.apply(lambda x: False if x > 30000 else True)])\n",
    "num1.set(xlabel = \"Shares (Raw, Unfiltered)\", title = \"Distribution of Shares\", ylabel = \"Number of Shares\")\n",
    "num2.set(xlabel = \"Shares (Truncated)\", title = \"Distribution of Shares\", ylabel = \"Number of Shares\")\n",
    "why = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371d102",
   "metadata": {},
   "source": [
    "There were 57 variables, some of which were likely unneeded. First, we took out variables that were highly multicorrelated, then we took out variables that were correlated with each other. We first did a coarse search for the right correlation to use, as shown below, before fine tuning the correct correlation to filter out variables while maintaining a good MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d995129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>correlation</th>\n",
       "      <th>diff_MAE</th>\n",
       "      <th>model</th>\n",
       "      <th>total_dropped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2376.217700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.585901</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2368.577658</td>\n",
       "      <td>0.1</td>\n",
       "      <td>21.945859</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2353.893151</td>\n",
       "      <td>0.2</td>\n",
       "      <td>7.261352</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2328.477566</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-18.154233</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2322.627474</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-24.004325</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2329.624071</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-17.007728</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2317.596518</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-29.035281</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2317.571526</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-29.060273</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2312.026782</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-34.605017</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2369.441261</td>\n",
       "      <td>0.9</td>\n",
       "      <td>22.809462</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2346.631799</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            MAE  correlation   diff_MAE               model  total_dropped\n",
       "0   2376.217700          0.0  29.585901  LinearRegression()           57.0\n",
       "1   2368.577658          0.1  21.945859  LinearRegression()           47.0\n",
       "2   2353.893151          0.2   7.261352  LinearRegression()           40.0\n",
       "3   2328.477566          0.3 -18.154233  LinearRegression()           29.0\n",
       "4   2322.627474          0.4 -24.004325  LinearRegression()           26.0\n",
       "5   2329.624071          0.5 -17.007728  LinearRegression()           20.0\n",
       "6   2317.596518          0.6 -29.035281  LinearRegression()           15.0\n",
       "7   2317.571526          0.7 -29.060273  LinearRegression()           15.0\n",
       "8   2312.026782          0.8 -34.605017  LinearRegression()            7.0\n",
       "9   2369.441261          0.9  22.809462  LinearRegression()            3.0\n",
       "10  2346.631799          1.0   0.000000  LinearRegression()            0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "correlation_range = [(x / 10) for x in range(0, 11, 1)]\n",
    "rmses = pd.DataFrame()\n",
    "base_model_mae = 2346.631798929844\n",
    "iteration = 0\n",
    "for correlation in correlation_range:\n",
    "    actual_columns_drop = []\n",
    "    columns_to_drop = []\n",
    "    X_train_copy = X_train.copy()\n",
    "    while (True):\n",
    "        try:\n",
    "            # Systematically remove variables one by one from dataset copy based on correlation, and break when no more variables to remove\n",
    "            correlations = X_train_copy.corr(numeric_only = True)\n",
    "            first_column = correlations.iloc[0].name\n",
    "            index = correlations[first_column].apply(lambda x: False if x == 1 else True if (np.abs(x) > correlation) else False)\n",
    "            to_drop = correlations[first_column].loc[index].index.tolist()\n",
    "            columns_to_drop.append(to_drop)\n",
    "            X_train_copy = X_train_copy.drop(columns = to_drop).drop(columns = first_column)\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    # Make column list of lists of columns into one list of columns\n",
    "    for columns in columns_to_drop:\n",
    "        for column in columns:\n",
    "            actual_columns_drop.append(column)\n",
    "            \n",
    "    # Create new model based off of new columns to drop, and then add the rmse to rmses\n",
    "    model = LinearRegression().fit(X_train.drop(columns = actual_columns_drop), np.log(y_train))\n",
    "    pred = model.predict(X_test.drop(columns = actual_columns_drop))\n",
    "    model_rmse = mean_absolute_error(y_test, np.exp(pred))\n",
    "    \n",
    "    rmses.loc[iteration, 'MAE'] = model_rmse\n",
    "    rmses.loc[iteration, 'correlation'] = correlation\n",
    "    rmses.loc[iteration, 'diff_MAE'] = model_rmse - base_model_mae\n",
    "    rmses.loc[iteration, 'model'] = model\n",
    "    rmses.loc[iteration, 'total_dropped'] = len(actual_columns_drop)\n",
    "    iteration += 1\n",
    "rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcdb7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "X_train = X_train.drop(columns = ['weekday_is_monday',\n",
    " 'weekday_is_saturday',\n",
    " 'LDA_00',\n",
    " 'n_unique_tokens',\n",
    " 'n_non_stop_words',\n",
    " 'self_reference_avg_sharess',\n",
    " 'rate_positive_words',\n",
    " 'LDA_04',\n",
    " 'LDA_02',\n",
    " 'kw_max_max',\n",
    " 'kw_avg_min',\n",
    " 'kw_avg_avg',\n",
    " 'rate_negative_words',\n",
    " 'min_negative_polarity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31ef8d40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were no missing values in the predictors used in our models. We also judged it unnecessary to make any investigations \n",
      "into any of the predictors used in model creation\n",
      "This is the distribution of the predictors used in our models\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "      <td>31715.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10.402617</td>\n",
       "      <td>546.024405</td>\n",
       "      <td>0.693708</td>\n",
       "      <td>10.893899</td>\n",
       "      <td>3.298408</td>\n",
       "      <td>4.566041</td>\n",
       "      <td>1.261958</td>\n",
       "      <td>4.549486</td>\n",
       "      <td>7.223301</td>\n",
       "      <td>0.051679</td>\n",
       "      <td>0.179064</td>\n",
       "      <td>0.156803</td>\n",
       "      <td>0.058868</td>\n",
       "      <td>0.187104</td>\n",
       "      <td>0.211162</td>\n",
       "      <td>26.284597</td>\n",
       "      <td>1137.575971</td>\n",
       "      <td>13576.556866</td>\n",
       "      <td>259212.551273</td>\n",
       "      <td>1117.526604</td>\n",
       "      <td>5649.083030</td>\n",
       "      <td>3965.305588</td>\n",
       "      <td>10139.459523</td>\n",
       "      <td>0.185086</td>\n",
       "      <td>0.187766</td>\n",
       "      <td>0.183793</td>\n",
       "      <td>0.143181</td>\n",
       "      <td>0.069242</td>\n",
       "      <td>0.130853</td>\n",
       "      <td>0.141131</td>\n",
       "      <td>0.224729</td>\n",
       "      <td>0.443705</td>\n",
       "      <td>0.119846</td>\n",
       "      <td>0.039660</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.354264</td>\n",
       "      <td>0.095802</td>\n",
       "      <td>0.757180</td>\n",
       "      <td>-0.259333</td>\n",
       "      <td>-0.107613</td>\n",
       "      <td>0.282917</td>\n",
       "      <td>0.071555</td>\n",
       "      <td>0.341392</td>\n",
       "      <td>0.156861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.111554</td>\n",
       "      <td>472.583153</td>\n",
       "      <td>3.649366</td>\n",
       "      <td>11.303247</td>\n",
       "      <td>3.826147</td>\n",
       "      <td>8.378235</td>\n",
       "      <td>4.100420</td>\n",
       "      <td>0.839615</td>\n",
       "      <td>1.911541</td>\n",
       "      <td>0.221382</td>\n",
       "      <td>0.383412</td>\n",
       "      <td>0.363620</td>\n",
       "      <td>0.235381</td>\n",
       "      <td>0.390001</td>\n",
       "      <td>0.408139</td>\n",
       "      <td>69.849697</td>\n",
       "      <td>3871.638621</td>\n",
       "      <td>58085.431099</td>\n",
       "      <td>135340.920102</td>\n",
       "      <td>1137.817833</td>\n",
       "      <td>6193.716511</td>\n",
       "      <td>19100.198864</td>\n",
       "      <td>38765.779360</td>\n",
       "      <td>0.388373</td>\n",
       "      <td>0.390531</td>\n",
       "      <td>0.387322</td>\n",
       "      <td>0.350263</td>\n",
       "      <td>0.253869</td>\n",
       "      <td>0.337245</td>\n",
       "      <td>0.219278</td>\n",
       "      <td>0.295801</td>\n",
       "      <td>0.116199</td>\n",
       "      <td>0.097044</td>\n",
       "      <td>0.017464</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>0.104438</td>\n",
       "      <td>0.071767</td>\n",
       "      <td>0.247253</td>\n",
       "      <td>0.127883</td>\n",
       "      <td>0.095513</td>\n",
       "      <td>0.324212</td>\n",
       "      <td>0.266481</td>\n",
       "      <td>0.188892</td>\n",
       "      <td>0.226995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.380208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.625566</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.478055</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>172502.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3563.797735</td>\n",
       "      <td>639.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.396621</td>\n",
       "      <td>0.058031</td>\n",
       "      <td>0.028379</td>\n",
       "      <td>0.009585</td>\n",
       "      <td>0.306522</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328593</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>408.000000</td>\n",
       "      <td>0.690987</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.663985</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>658.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>244716.666667</td>\n",
       "      <td>1024.692308</td>\n",
       "      <td>4353.136126</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.453349</td>\n",
       "      <td>0.119522</td>\n",
       "      <td>0.039007</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.359056</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.252889</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>0.755319</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.854293</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>7900.000000</td>\n",
       "      <td>330929.166667</td>\n",
       "      <td>2064.150077</td>\n",
       "      <td>6018.127965</td>\n",
       "      <td>2600.000000</td>\n",
       "      <td>8000.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151037</td>\n",
       "      <td>0.378857</td>\n",
       "      <td>0.508098</td>\n",
       "      <td>0.178668</td>\n",
       "      <td>0.050314</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.411598</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186574</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>8474.000000</td>\n",
       "      <td>650.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>3610.124972</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>690400.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.919984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       n_tokens_title  n_tokens_content  n_non_stop_unique_tokens  \\\n",
       "count    31715.000000      31715.000000              31715.000000   \n",
       "mean        10.402617        546.024405                  0.693708   \n",
       "std          2.111554        472.583153                  3.649366   \n",
       "min          2.000000          0.000000                  0.000000   \n",
       "25%          9.000000        246.000000                  0.625566   \n",
       "50%         10.000000        408.000000                  0.690987   \n",
       "75%         12.000000        714.000000                  0.755319   \n",
       "max         23.000000       8474.000000                650.000000   \n",
       "\n",
       "          num_hrefs  num_self_hrefs      num_imgs    num_videos  \\\n",
       "count  31715.000000    31715.000000  31715.000000  31715.000000   \n",
       "mean      10.893899        3.298408      4.566041      1.261958   \n",
       "std       11.303247        3.826147      8.378235      4.100420   \n",
       "min        0.000000        0.000000      0.000000      0.000000   \n",
       "25%        4.000000        1.000000      1.000000      0.000000   \n",
       "50%        7.000000        3.000000      1.000000      0.000000   \n",
       "75%       14.000000        4.000000      4.000000      1.000000   \n",
       "max      187.000000       74.000000    128.000000     75.000000   \n",
       "\n",
       "       average_token_length  num_keywords  data_channel_is_lifestyle  \\\n",
       "count          31715.000000  31715.000000               31715.000000   \n",
       "mean               4.549486      7.223301                   0.051679   \n",
       "std                0.839615      1.911541                   0.221382   \n",
       "min                0.000000      1.000000                   0.000000   \n",
       "25%                4.478055      6.000000                   0.000000   \n",
       "50%                4.663985      7.000000                   0.000000   \n",
       "75%                4.854293      9.000000                   0.000000   \n",
       "max                8.041534     10.000000                   1.000000   \n",
       "\n",
       "       data_channel_is_entertainment  data_channel_is_bus  \\\n",
       "count                   31715.000000         31715.000000   \n",
       "mean                        0.179064             0.156803   \n",
       "std                         0.383412             0.363620   \n",
       "min                         0.000000             0.000000   \n",
       "25%                         0.000000             0.000000   \n",
       "50%                         0.000000             0.000000   \n",
       "75%                         0.000000             0.000000   \n",
       "max                         1.000000             1.000000   \n",
       "\n",
       "       data_channel_is_socmed  data_channel_is_tech  data_channel_is_world  \\\n",
       "count            31715.000000          31715.000000           31715.000000   \n",
       "mean                 0.058868              0.187104               0.211162   \n",
       "std                  0.235381              0.390001               0.408139   \n",
       "min                  0.000000              0.000000               0.000000   \n",
       "25%                  0.000000              0.000000               0.000000   \n",
       "50%                  0.000000              0.000000               0.000000   \n",
       "75%                  0.000000              0.000000               0.000000   \n",
       "max                  1.000000              1.000000               1.000000   \n",
       "\n",
       "         kw_min_min     kw_max_min     kw_min_max     kw_avg_max  \\\n",
       "count  31715.000000   31715.000000   31715.000000   31715.000000   \n",
       "mean      26.284597    1137.575971   13576.556866  259212.551273   \n",
       "std       69.849697    3871.638621   58085.431099  135340.920102   \n",
       "min       -1.000000       0.000000       0.000000       0.000000   \n",
       "25%       -1.000000     445.000000       0.000000  172502.857143   \n",
       "50%       -1.000000     658.000000    1400.000000  244716.666667   \n",
       "75%        4.000000    1000.000000    7900.000000  330929.166667   \n",
       "max      377.000000  298400.000000  843300.000000  843300.000000   \n",
       "\n",
       "         kw_min_avg     kw_max_avg  self_reference_min_shares  \\\n",
       "count  31715.000000   31715.000000               31715.000000   \n",
       "mean    1117.526604    5649.083030                3965.305588   \n",
       "std     1137.817833    6193.716511               19100.198864   \n",
       "min       -1.000000       0.000000                   0.000000   \n",
       "25%        0.000000    3563.797735                 639.000000   \n",
       "50%     1024.692308    4353.136126                1200.000000   \n",
       "75%     2064.150077    6018.127965                2600.000000   \n",
       "max     3610.124972  298400.000000              690400.000000   \n",
       "\n",
       "       self_reference_max_shares  weekday_is_tuesday  weekday_is_wednesday  \\\n",
       "count               31715.000000        31715.000000          31715.000000   \n",
       "mean                10139.459523            0.185086              0.187766   \n",
       "std                 38765.779360            0.388373              0.390531   \n",
       "min                     0.000000            0.000000              0.000000   \n",
       "25%                  1100.000000            0.000000              0.000000   \n",
       "50%                  2800.000000            0.000000              0.000000   \n",
       "75%                  8000.000000            0.000000              0.000000   \n",
       "max                843300.000000            1.000000              1.000000   \n",
       "\n",
       "       weekday_is_thursday  weekday_is_friday  weekday_is_sunday  \\\n",
       "count         31715.000000       31715.000000       31715.000000   \n",
       "mean              0.183793           0.143181           0.069242   \n",
       "std               0.387322           0.350263           0.253869   \n",
       "min               0.000000           0.000000           0.000000   \n",
       "25%               0.000000           0.000000           0.000000   \n",
       "50%               0.000000           0.000000           0.000000   \n",
       "75%               0.000000           0.000000           0.000000   \n",
       "max               1.000000           1.000000           1.000000   \n",
       "\n",
       "         is_weekend        LDA_01        LDA_03  global_subjectivity  \\\n",
       "count  31715.000000  31715.000000  31715.000000         31715.000000   \n",
       "mean       0.130853      0.141131      0.224729             0.443705   \n",
       "std        0.337245      0.219278      0.295801             0.116199   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.000000      0.025012      0.028571             0.396621   \n",
       "50%        0.000000      0.033345      0.040001             0.453349   \n",
       "75%        0.000000      0.151037      0.378857             0.508098   \n",
       "max        1.000000      0.925947      0.919984             1.000000   \n",
       "\n",
       "       global_sentiment_polarity  global_rate_positive_words  \\\n",
       "count               31715.000000                31715.000000   \n",
       "mean                    0.119846                    0.039660   \n",
       "std                     0.097044                    0.017464   \n",
       "min                    -0.380208                    0.000000   \n",
       "25%                     0.058031                    0.028379   \n",
       "50%                     0.119522                    0.039007   \n",
       "75%                     0.178668                    0.050314   \n",
       "max                     0.727841                    0.155488   \n",
       "\n",
       "       global_rate_negative_words  avg_positive_polarity  \\\n",
       "count                31715.000000           31715.000000   \n",
       "mean                     0.016598               0.354264   \n",
       "std                      0.010857               0.104438   \n",
       "min                      0.000000               0.000000   \n",
       "25%                      0.009585               0.306522   \n",
       "50%                      0.015337               0.359056   \n",
       "75%                      0.021739               0.411598   \n",
       "max                      0.184932               1.000000   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "count           31715.000000           31715.000000           31715.000000   \n",
       "mean                0.095802               0.757180              -0.259333   \n",
       "std                 0.071767               0.247253               0.127883   \n",
       "min                 0.000000               0.000000              -1.000000   \n",
       "25%                 0.050000               0.600000              -0.328593   \n",
       "50%                 0.100000               0.800000              -0.252889   \n",
       "75%                 0.100000               1.000000              -0.186574   \n",
       "max                 1.000000               1.000000               0.000000   \n",
       "\n",
       "       max_negative_polarity  title_subjectivity  title_sentiment_polarity  \\\n",
       "count           31715.000000        31715.000000              31715.000000   \n",
       "mean               -0.107613            0.282917                  0.071555   \n",
       "std                 0.095513            0.324212                  0.266481   \n",
       "min                -1.000000            0.000000                 -1.000000   \n",
       "25%                -0.125000            0.000000                  0.000000   \n",
       "50%                -0.100000            0.150000                  0.000000   \n",
       "75%                -0.050000            0.500000                  0.150000   \n",
       "max                 0.000000            1.000000                  1.000000   \n",
       "\n",
       "       abs_title_subjectivity  abs_title_sentiment_polarity  \n",
       "count            31715.000000                  31715.000000  \n",
       "mean                 0.341392                      0.156861  \n",
       "std                  0.188892                      0.226995  \n",
       "min                  0.000000                      0.000000  \n",
       "25%                  0.166667                      0.000000  \n",
       "50%                  0.500000                      0.000000  \n",
       "75%                  0.500000                      0.250000  \n",
       "max                  0.500000                      1.000000  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(\"There were no missing values in the predictors used in our models. We also judged it unnecessary to make any investigations \\ninto any of the predictors used in model creation\")\n",
    "print(\"This is the distribution of the predictors used in our models\")\n",
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb11c9b",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68356589",
   "metadata": {},
   "source": [
    "* The only insight is to put a log transformation on the response variable (shares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39c782c",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "What kind of a models did you use? What performance metric(s) did you optimize and why?\n",
    "\n",
    "Is there anything unorthodox / new in your approach? \n",
    "\n",
    "What problems did you anticipate? What problems did you encounter? \n",
    "\n",
    "Did your problem already have solution(s) (posted on Kaggle or elsewhere). If yes, then how did you build upon those solutions, what did you do differently? Is your model better as compared to those solutions in terms of prediction accuracy or your chosen metric?\n",
    "\n",
    "**Important: Mention any code repositories (with citations) or other sources that you used, and specifically what changes you made to them for your project.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8a3bd4",
   "metadata": {},
   "source": [
    "We used MARS, CatBoost, XGBoost, and Random Forest models. We optimized MAE, because a large error in model prediction is not significantly worse than a small error in model prediction. Moreover, we did not turn our problem into a classification problem, as we hoped to be able to use our models to predict the relative success of an article, and not just use an arbitrary threshold to predict whether an article is \"successful\" or a \"failure\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab331a",
   "metadata": {},
   "source": [
    "## Developing the model: Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ec4c9",
   "metadata": {},
   "source": [
    "Every person must describe their hyperparameter tuning procedure. Show the grid of hyperparameter values over which the initial search was done *(you may paste your grid search / random search / any other search code)*, and the optimal hyperparameter values obtained. After getting the initial search results, how did you make decisions *(if any)* to further fine-tune your model. Did you do another grid / random search or did you tune hyperparameters sequentially? If you think you didn't need any fine tuning after the initial results, then mention that and explain why.\n",
    "\n",
    "Put each model in a section of its name and mention the name of the team-member tuning the model. Below is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea422beb",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "*By Jack O'Keefe*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc924e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, Trials, fmin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "x_train_encoded = X_train.copy()\n",
    "y_train = y.copy()\n",
    "\n",
    "# Define the objective function\n",
    "def objective(params):\n",
    "    model = RandomForestRegressor(\n",
    "        **params,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    score = cross_val_score(model, X_train, np.log(y_train), cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    return -score\n",
    "\n",
    "# Define the search space\n",
    "space = {\n",
    "    'n_estimators': hp.choice('n_estimators', range(10, 500)),\n",
    "    'max_depth': hp.choice('max_depth', range(1, 9)),\n",
    "    'min_samples_split': hp.choice('min_samples_split', range(2, 11)),\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 11)),\n",
    "    'max_features': hp.choice('max_features', ['auto', 'sqrt', 'log2'])\n",
    "}\n",
    "\n",
    "# Run the optimization\n",
    "trials = Trials()\n",
    "best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba7ed01",
   "metadata": {},
   "source": [
    "#### Optimal Hyperparameters \n",
    "\n",
    "{'max_depth': 7, 'max_features': 0, 'min_samples_leaf': 9, 'min_samples_split': 3, 'n_estimators': 287}\n",
    "\n",
    "I first created an untuned random forest and saw the hyperparameter values that were produced from it. That guided by base tuning. I used bayesian optimization because it is much faster than the traditional grid search. This is because it learns from the mistakes of the previous assumptions to more quickly make decisions on the optimal hyperparameters. I ran this twice. \n",
    "\n",
    "I was unable to compare my accuracy to the result in the published paper as they made this a classification problem, but the random forest performed better than our other models, initially. This illustrates how there is no one model that is perfectly suited to every dataset. In that case, someone could use XGBoost or CatBoost on every model and hope for the best result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78fbd0e",
   "metadata": {},
   "source": [
    "### MARS & Catboost \n",
    "*By Abenezer Bekele*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b4bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from pyearth import Earth\n",
    "\n",
    "# Define the model\n",
    "mars_model = Earth()\n",
    "\n",
    "# Define the parameters\n",
    "parameters = {'max_terms': [300,500,1000],\n",
    "              'max_degree': [1,2,3,4]}\n",
    "\n",
    "# Define the cross-validator\n",
    "CV = KFold(n_splits = 5, shuffle=True, random_state=1)\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "model = GridSearchCV(estimator=mars_model, \n",
    "                     param_grid=parameters, \n",
    "                     cv=CV, \n",
    "                     n_jobs=-1, \n",
    "                     verbose=1, \n",
    "                     scoring='neg_mean_absolute_error')\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "model.fit(X_train, y_train)  \n",
    "\n",
    "# Print the best score and best parameters\n",
    "print('Best score:', model.best_score_)\n",
    "print('Best parameters:', model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296cdc0",
   "metadata": {},
   "source": [
    "I utilized the GridSearchCV class from sklearn to optimize the hyperparameters of a MARS model. I first defined the model using the Earth class from pyearth. I set the hyperparameters to be tuned - 'max_terms' and 'max_degree'. I also set up a 5-fold cross-validation using KFold to evaluate the model. I then initialized GridSearchCV with the MARS model, the hyperparameters, cross-validator, and scoring metric (negative mean absolute error). After fitting GridSearchCV on the training data, I printed the best score and the best hyperparameters to understand the model's performance and configuration. The best hyperparameters are {'max_degree': 1, 'max_terms': 300}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6de4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_model = Earth(max_terms = 300, max_degree = 1)\n",
    "mars_model = TransformedTargetRegressor(mars_model, func = np.log, inverse_func = np.exp).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c027a20",
   "metadata": {},
   "source": [
    "I then instantiated a MARS model with specified hyperparameters, wrapped it in TransformedTargetRegressor for target transformation, and fit it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7e52e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-fold cross validation to find optimal parameters for CatBoost regressor\n",
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8],\n",
    "              'num_leaves': [20, 31, 40],\n",
    "              'learning_rate': [0.01, 0.05, 0.1],\n",
    "               'reg_lambda':[0, 10, 100],\n",
    "                'n_estimators':[100, 500, 1000],\n",
    "                'subsample': [0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=CatBoostRegressor(random_state=1, verbose=False),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,random_state = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X_train,y_train)\n",
    "print(\"Optimal parameter values =\", optimal_params.best_params_)\n",
    "print(\"Optimal cross validation R-squared = \",optimal_params.best_score_)\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaee5a2",
   "metadata": {},
   "source": [
    "I initiated hyperparameter tuning for a CatBoost Regressor using RandomizedSearchCV. I defined a grid for parameters including 'max_depth', 'num_leaves', 'learning_rate', 'reg_lambda', 'n_estimators', and 'subsample'. I used a 5-fold cross-validation with KFold. The RandomizedSearchCV object was configured with 200 iterations, and parallel processing enabled. After fitting it to the training data, I printed the best hyperparameters, the best cross-validation R-squared score, and the time taken for the whole process in minutes. The scores I got are as follows:{'subsample': 0.75, 'reg_lambda': 10, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.01}. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model with the provided parameters\n",
    "cat_model = CatBoostRegressor(subsample = 0.75, reg_lambda=10, \n",
    "                              depth=6, \n",
    "                              n_estimators=1000, \n",
    "                              learning_rate=0.01,\n",
    "                              random_state=1, \n",
    "                              verbose=False)\n",
    "\n",
    "# Fit the model to your training data\n",
    "cat_model= TransformedTargetRegressor(cat_model, func = np.log, inverse_func = np.exp).fit(X_train, y_train)\n",
    "\n",
    "# Predict on your test set\n",
    "y_pred = cat_model.predict(X_test)  \n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)  \n",
    "\n",
    "print('Mean Absolute Error:', mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4df15d",
   "metadata": {},
   "source": [
    "I then created a CatBoost Regressor with specified hyperparameters and wrapped it in a TransformedTargetRegressor for transforming the target variable using logarithm and exponential functions. After fitting the model to the training data, I made predictions on the test set and calculated the Mean Absolute Error (MAE) to evaluate the model's performance, which was then printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916849c",
   "metadata": {},
   "source": [
    "### Bagged trees & Random forest\n",
    "*By Fiona Fe*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f552ef",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "*By Ryu Kimiko*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2594f1",
   "metadata": {},
   "source": [
    "## Model Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68b0a5",
   "metadata": {},
   "source": [
    "Put the results of enembling individual models. Feel free to add subsections in this section to add more innovative ensembling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5924a",
   "metadata": {},
   "source": [
    "### Voting ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a31cb2",
   "metadata": {},
   "source": [
    "The simplest voting ensemble will be the model where all models have equal weights.\n",
    "\n",
    "You may come up with innovative methods of estimating weights of the individual models, such as based on their cross-val error. Sometimes, these methods may work better than stacking ensembles, as stacking ensembles tend to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcff4cda",
   "metadata": {},
   "source": [
    "### Stacking ensemble\n",
    "Try out different models as the metamodel. You may split work as follows. The person who worked on certain types of models *(say AdaBoost and MARS)* also uses those models as a metamodel in the stacking ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22a5f3",
   "metadata": {},
   "source": [
    "### Ensemble of ensembled models\n",
    "\n",
    "If you are creating multiple stacking ensembles *(based on different metamodels)*, you may ensemble them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe5f5f",
   "metadata": {},
   "source": [
    "### Innovative ensembling methods\n",
    "*(Optional)*\n",
    "\n",
    "Some models may do better on certain subsets of the predictor space. You may find that out, and given a data point, choose the model(s) that will best predict for that data point. This is similar to the idea of developing a decision tree metamodel. However, decision tree is prone to overfitting.\n",
    "\n",
    "Another idea may be to correct the individual models with the intercept and slope *(note the tree-based models don't have an intercept and may suffer from a constant bias)*, and then ensemble them. This is equivalent to having a simple linear regression meta-model for each of the individual models, and then ensembling the meta-models with a meta-metamodel or a voting ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b46343d",
   "metadata": {},
   "source": [
    "## Limitations of the model with regard to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ead90",
   "metadata": {},
   "source": [
    "Are you confident that you found the optimal hyperparameter values for each of your individual models, and that your individual models cannot be better tuned? Or, are there any models that could be better tuned if you had more time / resources, but you are limited by the amount of time you can spend on the course project *(equivalent to one assignment)*? If yes, then which models could be better tuned and how?\n",
    "\n",
    "Will it be possible / convenient / expensive for the stakeholders to collect the data relating to the predictors in the model. Using your model, how soon will the stakeholder be able to predict the outcome before the outcome occurs. For example, if the model predicts the number of bikes people will rent in Evanston on a certain day, then how many days before that day will your model be able to make the prediction. This will depend on how soon the data that your model uses becomes available. If you are predicting election results, how many days / weeks / months / years before the election can you predict the results. \n",
    "\n",
    "When will your model become too obsolete to be useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6026cb7",
   "metadata": {},
   "source": [
    "## Other sections *(optional)*\n",
    "\n",
    "You are welcome to introduce additional sections or subsections, if required, to address any specific aspects of your project in detail. For example, you may briefly discuss potential future work that the research community could focus on to make further progress in the direction of your project's topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a185cb",
   "metadata": {},
   "source": [
    "## Conclusions and Recommendations to stakeholder(s)\n",
    "\n",
    "What conclusions do you draw based on your model? You may draw conclusions based on prediction accuracy, or other performance metrics.\n",
    "\n",
    "How do you use those conclusions to come up with meaningful recommendations for stakeholders? The recommendations must be action-items for stakeholders that they can directly implement without any further analysis. Be as precise as possible. The stakeholder(s) are depending on you to come up with practically implementable recommendations, instead of having to think for themselves.\n",
    "\n",
    "If your recommendations are not practically implementable by stakeholders, how will they help them? Is there some additional data / analysis / domain expertise you need to do to make the recommendations implementable? \n",
    "\n",
    "Do the stakeholder(s) need to be aware about some limitations of your model? Is your model only good for one-time use, or is it possible to update your model at a certain frequency (based on recent data) to keep using it in the future? If it can be used in the future, then for how far into the future?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e943cb",
   "metadata": {},
   "source": [
    "Based on our model, it becomes evident that we did not bring the MAE down to a level where one would be able to make fine, accurate predictions. However, our model would be very useful in being able to predict the general range of popularity an article woudld lie in. For instance, if our model predicted shares of roughly 20,000, it would be clear that that particular article would be relatively popular, regardless of the relative error of the prediction. However, if our model predicts shares in the range of 1000, regardless of the relative error of the prediction it is clear that some work should probably be done on the article to maximize engagement. \n",
    "\n",
    "Therefore, what stakeholders should do is use the model to make predictions on a particularly divisive or questionable article to measure the relative success that article would have. Then, depending on the predicted number of shares, and the range of shares the particular firm would be looking to land in, they can decide whether the article is able to be published as is, or needs more work in order to gain more engagement. In this way, they will be able to have relatively consistent high-engagement articles that will drive more readers to their site, increasing their reputation as a publisher with consistently good articles that are worth the reader's time. What would be best to do, however, is use articles written only by the publisher, in order to customize the models towards the publisher's own peculiarities in writing. These more specialized models will likely be able to more accurate predict the publisher's article success, due to the more personable data.\n",
    "\n",
    "Our model is not only good for a one-time use. Each new batch of recent data from the publisher will be able to be added to the database in order to fit new models to create more accurate predictions. Moreover, given that the predictors used in the models are highly generalizabel to all articles, it will likely be able to be used until articles are no longer being written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca45613",
   "metadata": {},
   "source": [
    "Add details of each team member's contribution, other than the models contributed, in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5505da5c",
   "metadata": {},
   "source": [
    "<html>\n",
    "<style>\n",
    "table, td, th {\n",
    "  border: 1px solid black;\n",
    "}\n",
    "\n",
    "table {\n",
    "  border-collapse: collapse;\n",
    "  width: 100%;\n",
    "}\n",
    "\n",
    "th {\n",
    "  text-align: left;\n",
    "}\n",
    "    \n",
    "\n",
    "</style>\n",
    "<body>\n",
    "\n",
    "<h2>Individual contribution</h2>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <colgroup>\n",
    "       <col span=\"1\" style=\"width: 15%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 25%;\">\n",
    "       <col span=\"1\" style=\"width: 40%;\">\n",
    "    </colgroup>\n",
    "  <tr>\n",
    "    <th>Team member</th>\n",
    "    <th>Individual Model</th>\n",
    "    <th>Work other than individual model</th>    \n",
    "    <th>Details of work other than individual model</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Nathan Jung</td>\n",
    "    <td>XGBoost</td>\n",
    "    <td></td>    \n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Abenezer Bekele</td>\n",
    "    <td>MARS, CatBoost</td>\n",
    "    <td></td>    \n",
    "    <td> </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>Jack O'Keefe</td>\n",
    "    <td>Random forest</td>\n",
    "    <td></td>    \n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1cafe",
   "metadata": {},
   "source": [
    "## References {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdb1aad",
   "metadata": {},
   "source": [
    "List and number all bibliographical references. When referenced in the text, enclose the citation number in square brackets, for example [1].\n",
    "\n",
    "[1] Authors. The frobnicatable foo filter, 2014. Face and Gesture submission ID 324. Supplied as additional material\n",
    "fg324.pdf. 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5831751c",
   "metadata": {},
   "source": [
    "## Appendix {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13d374d",
   "metadata": {},
   "source": [
    "You may put additional stuff here as Appendix. You may refer to the Appendix in the main report to support your arguments. However, the appendix section is unlikely to be checked while grading, unless the grader deems it necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
